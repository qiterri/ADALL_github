{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qiterri/ADALL_github/blob/main/ADALL_Classification_FeatureImportance_with_github_push.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50474190",
      "metadata": {
        "id": "50474190"
      },
      "source": [
        "# ADALL (Project-Focused):\n",
        "\n",
        "## Classification → Feature Importance → Permutation Feature Importance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "teloijwwG1-P",
      "metadata": {
        "id": "teloijwwG1-P"
      },
      "source": [
        "## Feature Importance (FI) from a decision tree\n",
        "\n",
        "You already know a decision tree is a set of questions (if-then-else) that ends in a decision (prediction).\n",
        "The problem is: once the tree becomes deeper, it is hard to tell which questions (features) mattered most overall.\n",
        "\n",
        "Below is a **visual decision tree**.  \n",
        "Notice how quickly it becomes hard to reason about importance by inspection alone. It might be easy to understand that `budget` is the most important in laptop, but it may not be so for `gaming` and `dedicated GPU`.\n",
        "\n",
        "```text\n",
        "                                      Start\n",
        "                                        |\n",
        "                           Is budget < $1500?\n",
        "                             /             \\\n",
        "                           Yes              No\n",
        "                           |                |\n",
        "               Is gaming the main use?   Has a dedicated GPU?\n",
        "                 /         \\                /         \\\n",
        "               Yes         No             Yes          No\n",
        "               |           |              |            |\n",
        "     Has a dedicated GPU?  Need long     Suggest Pro   Suggest Value\n",
        "         /      \\          battery?\n",
        "       Yes      No         /     \\\n",
        "       |        |        Yes      No\n",
        "   Suggest G1  Suggest G2  |\n",
        "                        Is gaming the main use?\n",
        "                          /        \\\n",
        "                        Yes        No\n",
        "                        |           |\n",
        "                   Suggest U1    Suggest U2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8V9OludDLdNq",
      "metadata": {
        "id": "8V9OludDLdNq"
      },
      "source": [
        "## How Feature Importance (FI) summarises the tree into an overall ranking\n",
        "\n",
        "Feature Importance (FI) is **not** simply “counting how often a question appears”, and it is not a perfect “true ranking” either.\n",
        "\n",
        "In tree models, FI is based on **improvement** at splits.\n",
        "\n",
        "---\n",
        "\n",
        "### What FI actually measures (tree example)\n",
        "\n",
        "Each time the tree asks a question (feature), it makes a split.\n",
        "\n",
        "A “good” split is one that improves the quality of separation between classes, for example:\n",
        "- the child nodes become more pure (less mixed classes)\n",
        "- the model becomes more confident about the predicted class\n",
        "\n",
        "In many tree implementations, this improvement is measured using something like:\n",
        "- **Gini decrease** (classification), or\n",
        "- **entropy decrease** (information gain)\n",
        "\n",
        "So FI is closer to:\n",
        "\n",
        "> “How much did this feature improve the splits, summed over where it was used?”\n",
        "\n",
        "This is why FI is usually described as:\n",
        "- **total decrease in impurity** contributed by the feature (often averaged across trees in a forest)\n",
        "\n",
        "---\n",
        "\n",
        "### Why FI is easier than reading the full tree\n",
        "\n",
        "Even in your laptop tree, you would need to track:\n",
        "- where each feature is used\n",
        "- how strong each split is\n",
        "- how many records flow through that split\n",
        "\n",
        "Once the model becomes deeper, or becomes a forest (many trees), this is impossible to do manually.\n",
        "\n",
        "FI compresses all those split-level improvements into a short list like:\n",
        "\n",
        "1. `budget`\n",
        "2. `gaming`\n",
        "3. `dedicated GPU`\n",
        "4. `battery`\n",
        "5. `weight`\n",
        "\n",
        "This list is **a true ordinal ranking** of the model’s *split-improvement contributions*.\n",
        "\n",
        "---\n",
        "\n",
        "### Why FI values are ordinal, not numeric in meaning\n",
        "\n",
        "FI usually produces numbers, for example:\n",
        "\n",
        "| Feature | FI |\n",
        "|-------|----|\n",
        "| budget | 0.42 |\n",
        "| gaming | 0.27 |\n",
        "| dedicated GPU | 0.19 |\n",
        "| battery | 0.12 |\n",
        "\n",
        "It is tempting to treat these numbers mathematically, but this is **not correct**.\n",
        "\n",
        "Important points:\n",
        "\n",
        "- The FI values are **relative**, not absolute\n",
        "- Adding them does not represent a real-world quantity  \n",
        "  (0.27 + 0.19 has no interpretable meaning)\n",
        "- The scale is model-dependent and dataset-dependent\n",
        "\n",
        "For example:\n",
        "- An FI of 0.42 in one model does **not** mean the feature is “twice as important” as 0.21 in another model\n",
        "- An FI of 0.30 in one dataset cannot be compared directly with 0.30 from another dataset\n",
        "\n",
        "This is because FI numbers come from **internal split improvements**, not from a universal unit.\n",
        "\n",
        "---\n",
        "\n",
        "### What you *can* and *cannot* do with FI numbers\n",
        "\n",
        "You **can**:\n",
        "- compare features **within the same model**\n",
        "- rank features from more important to less important\n",
        "- identify dominant or weak signals\n",
        "\n",
        "You **cannot**:\n",
        "- add FI values together\n",
        "- compare FI values across different models or datasets\n",
        "- interpret FI as probability, impact, or effect size\n",
        "\n",
        "This is why FI should be treated as a **true ordinal ranking tool**, not a precise numerical measurement.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4681eb2e",
      "metadata": {
        "id": "4681eb2e"
      },
      "source": [
        "---\n",
        "##Chapter 0) Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U \\\n",
        "  scikit-learn==1.8.0"
      ],
      "metadata": {
        "id": "aQmONNijXBbY"
      },
      "id": "aQmONNijXBbY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd5ae6b",
      "metadata": {
        "id": "1bd5ae6b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "print(\"Setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(\"scikit-learn version:\", sklearn.__version__)\n"
      ],
      "metadata": {
        "id": "Z7z9hoLTXMU_"
      },
      "id": "Z7z9hoLTXMU_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "457171d4",
      "metadata": {
        "id": "457171d4"
      },
      "source": [
        "##Chapter 1) Load dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a8230d",
      "metadata": {
        "id": "51a8230d"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer(as_frame=True)\n",
        "df = data.frame.copy()\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09b1e276",
      "metadata": {
        "id": "09b1e276"
      },
      "source": [
        "---\n",
        "##Chapter 2) Quick inspection with LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d7853a",
      "metadata": {
        "id": "44d7853a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "# ---------------------------\n",
        "# Generate a full dataset profile\n",
        "# ---------------------------\n",
        "buffer = StringIO()\n",
        "\n",
        "# dtypes\n",
        "buffer.write(\"=== DTYPES ===\\n\")\n",
        "buffer.write(df.dtypes.to_string())\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# numeric describe\n",
        "buffer.write(\"=== NUMERIC DESCRIBE ===\\n\")\n",
        "buffer.write(df.describe().to_string())\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# categorical describe\n",
        "buffer.write(\"=== CATEGORICAL DESCRIBE ===\\n\")\n",
        "try:\n",
        "    buffer.write(df.describe(include='object').to_string())\n",
        "except:\n",
        "    buffer.write(\"No categorical columns\")\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# null summary\n",
        "buffer.write(\"=== NULL SUMMARY ===\\n\")\n",
        "null_summary = (\n",
        "    df.isna().sum().to_frame(\"null_count\")\n",
        "    .assign(null_pct=lambda x: x[\"null_count\"]/len(df))\n",
        ")\n",
        "buffer.write(null_summary.to_string())\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# unique cardinality\n",
        "buffer.write(\"=== UNIQUE VALUES PER COLUMN ===\\n\")\n",
        "buffer.write(df.nunique().to_frame(\"unique_count\").to_string())\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# correlation matrix\n",
        "buffer.write(\"=== CORRELATIONS (NUMERIC ONLY) ===\\n\")\n",
        "buffer.write(df.corr(numeric_only=True).round(3).to_string())\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# value counts for categoricals\n",
        "buffer.write(\"=== VALUE COUNTS (TOP 20 PER CATEGORICAL COLUMN) ===\\n\")\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "if len(cat_cols) > 0:\n",
        "    for col in cat_cols:\n",
        "        buffer.write(f\"\\nColumn: {col}\\n\")\n",
        "        vc = df[col].value_counts().head(20)\n",
        "        buffer.write(vc.to_string())\n",
        "        buffer.write(\"\\n\")\n",
        "else:\n",
        "    buffer.write(\"No categorical columns\\n\")\n",
        "buffer.write(\"\\n\")\n",
        "\n",
        "# --------- FIXED OUTLIER COMPUTATION (NO BOOLEANS) ---------\n",
        "buffer.write(\"=== OUTLIER SUMMARY (IQR METHOD) ===\\n\")\n",
        "num_cols = df.select_dtypes(include=['number']).columns  # exclude booleans\n",
        "Q1 = df[num_cols].quantile(0.25)\n",
        "Q3 = df[num_cols].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = ((df[num_cols] < (Q1 - 1.5*IQR)) | (df[num_cols] > (Q3 + 1.5*IQR))).sum()\n",
        "buffer.write(outliers.to_string())\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# leakage scan: columns with all unique values\n",
        "buffer.write(\"=== POSSIBLE LEAKAGE COLUMNS (UNIQUE FOR EACH ROW) ===\\n\")\n",
        "leak_cols = df.columns[df.nunique() == len(df)]\n",
        "buffer.write(str(list(leak_cols)))\n",
        "buffer.write(\"\\n\\n\")\n",
        "\n",
        "# shape, duplicates, constant cols\n",
        "buffer.write(\"=== SHAPE / DUPLICATES / CONSTANT COLUMNS ===\\n\")\n",
        "dup_count = df.duplicated().sum()\n",
        "constant_cols = df.columns[df.nunique() == 1].tolist()\n",
        "buffer.write(f\"Rows: {len(df)}, Columns: {df.shape[1]}\\n\")\n",
        "buffer.write(f\"Duplicate rows: {dup_count}\\n\")\n",
        "buffer.write(f\"Constant columns: {constant_cols}\\n\\n\")\n",
        "\n",
        "# Final text\n",
        "payload_text = buffer.getvalue()\n",
        "\n",
        "print(payload_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "# Load key from Google Colab Secrets\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        ")"
      ],
      "metadata": {
        "id": "sVODEZa5Uds7"
      },
      "id": "sVODEZa5Uds7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5Tj70wwR9aTG",
      "metadata": {
        "id": "5Tj70wwR9aTG"
      },
      "outputs": [],
      "source": [
        "# response = client.responses.create(\n",
        "#     model=\"gpt-5-mini\",\n",
        "#     instructions=\"\"\"\n",
        "# You are an expert data scientist with extensive knowledge of tree-based models.\n",
        "# Always justify recommendations using reasoning trace based ONLY on the dataset profile.\n",
        "# \"\"\",\n",
        "#     input=f\"\"\"\n",
        "# Dataset info: {payload_text}\\n\n",
        "# Questions:\\n\n",
        "# 1. Based on the dataset profile, what data quality issues should be resolved before modelling?\n",
        "# Provide a priority list and justify each item. \\n\n",
        "# 2. Which columns appear redundant, correlated, or likely to cause leakage?\n",
        "# Explain why each is problematic. \\n\n",
        "# Next: Provide a python script to handle the identified issues.\n",
        "# Define one helper function for each issue.\n",
        "# Then define a wrapper function that calls these helper with true false option as user choice\n",
        "# Provide a single line of code to run the overall wrapper function.\n",
        "# Do not encode categorical columns or model first.\n",
        "# \"\"\")\n",
        "# print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "blCnxdGaHWT0",
      "metadata": {
        "id": "blCnxdGaHWT0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "\n",
        "def check_missing_and_duplicates(df, verbose=True):\n",
        "    \"\"\"Check for missing values and duplicates. Returns df unchanged but raises if missing present when verbose=True.\"\"\"\n",
        "    missing = df.isnull().sum()\n",
        "    total_missing = missing.sum()\n",
        "    if verbose:\n",
        "        print(\"Total missing values:\", int(total_missing))\n",
        "        if total_missing > 0:\n",
        "            print(missing[missing > 0])\n",
        "        dup_count = df.duplicated(keep=False).sum()\n",
        "        print(\"Duplicate rows count:\", int(dup_count))\n",
        "    return df\n",
        "\n",
        "def drop_constant_columns(df, verbose=True):\n",
        "    \"\"\"Drop any constant columns (zero variance).\"\"\"\n",
        "    nunique = df.nunique(dropna=False)\n",
        "    const_cols = nunique[nunique <= 1].index.tolist()\n",
        "    if verbose:\n",
        "        print(\"Constant columns to drop:\", const_cols)\n",
        "    if const_cols:\n",
        "        df = df.drop(columns=const_cols)\n",
        "    return df\n",
        "\n",
        "def remove_highly_correlated(df, target_col='target', threshold=0.95, verbose=True):\n",
        "    \"\"\"\n",
        "    Remove highly correlated features (pairwise abs corr > threshold).\n",
        "    For each correlated pair, keep the column with larger absolute Pearson correlation with target_col.\n",
        "    \"\"\"\n",
        "    # work on numeric columns only\n",
        "    numeric = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if target_col not in numeric:\n",
        "        raise ValueError(\"target_col must be numeric and present in df\")\n",
        "    features = [c for c in numeric if c != target_col]\n",
        "    corr = df[features].corr().abs()\n",
        "\n",
        "    to_drop = set()\n",
        "    # precompute abs correlation with target\n",
        "    abs_corr_with_target = df[features].corrwith(df[target_col]).abs()\n",
        "\n",
        "    # iterate upper triangle pairs\n",
        "    cols = corr.columns\n",
        "    for i in range(len(cols)):\n",
        "        for j in range(i+1, len(cols)):\n",
        "            c1 = cols[i]; c2 = cols[j]\n",
        "            if corr.iloc[i, j] > threshold:\n",
        "                # compare correlation with target; if equal use variance as tie-breaker (drop the lower variance)\n",
        "                corr1 = abs_corr_with_target.get(c1, 0.0)\n",
        "                corr2 = abs_corr_with_target.get(c2, 0.0)\n",
        "                if corr1 > corr2:\n",
        "                    drop_col = c2\n",
        "                elif corr2 > corr1:\n",
        "                    drop_col = c1\n",
        "                else:\n",
        "                    # tie-breaker: drop the one with smaller variance\n",
        "                    var1 = df[c1].var()\n",
        "                    var2 = df[c2].var()\n",
        "                    drop_col = c1 if var1 < var2 else c2\n",
        "                # don't drop target accidentally\n",
        "                if drop_col != target_col:\n",
        "                    to_drop.add(drop_col)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Removing {len(to_drop)} highly correlated columns (threshold={threshold}):\", sorted(list(to_drop)))\n",
        "    df = df.drop(columns=list(to_drop))\n",
        "    return df\n",
        "\n",
        "def cap_outliers_iqr(df, multiplier=1.5, exclude=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Cap values outside [Q1 - multiplier*IQR, Q3 + multiplier*IQR] for numeric columns.\n",
        "    exclude: list of columns to exclude from capping (e.g., target_col). By default exclude 'target' if present.\n",
        "    \"\"\"\n",
        "    if exclude is None:\n",
        "        exclude = []\n",
        "    if 'target' in df.columns and 'target' not in exclude:\n",
        "        exclude = exclude + ['target']\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cols_to_cap = [c for c in numeric_cols if c not in exclude]\n",
        "    capped_info = {}\n",
        "    for col in cols_to_cap:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower = Q1 - multiplier * IQR\n",
        "        upper = Q3 + multiplier * IQR\n",
        "        before_min = df[col].min()\n",
        "        before_max = df[col].max()\n",
        "        df[col] = df[col].clip(lower=lower, upper=upper)\n",
        "        after_min = df[col].min()\n",
        "        after_max = df[col].max()\n",
        "        capped_info[col] = (before_min, before_max, after_min, after_max)\n",
        "    if verbose:\n",
        "        # Print a brief summary for columns that changed ranges\n",
        "        changed = {k:v for k,v in capped_info.items() if v[0]!=v[2] or v[1]!=v[3]}\n",
        "        print(\"Columns with range changes after IQR capping:\", list(changed.keys()))\n",
        "    return df\n",
        "\n",
        "def resample_classes(df, target_col='target', method='none', random_state=42, verbose=True):\n",
        "    \"\"\"\n",
        "    Resample dataset to address class imbalance.\n",
        "    method options: 'none' | 'oversample' | 'undersample'\n",
        "    - oversample: upsample minority class to match majority\n",
        "    - undersample: downsample majority class to match minority\n",
        "    Returns a new DataFrame (shuffled).\n",
        "    \"\"\"\n",
        "    if method not in ('none', 'oversample', 'undersample'):\n",
        "        raise ValueError(\"method must be one of 'none','oversample','undersample'\")\n",
        "    if method == 'none':\n",
        "        if verbose:\n",
        "            print(\"No resampling performed.\")\n",
        "        return df.sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "    # split classes\n",
        "    classes = df[target_col].unique().tolist()\n",
        "    if len(classes) != 2:\n",
        "        raise ValueError(\"resample_classes currently supports binary target only\")\n",
        "    class0 = df[df[target_col] == classes[0]]\n",
        "    class1 = df[df[target_col] == classes[1]]\n",
        "    # determine majority/minority\n",
        "    if len(class0) > len(class1):\n",
        "        majority, minority = class0, class1\n",
        "    else:\n",
        "        majority, minority = class1, class0\n",
        "\n",
        "    if method == 'oversample':\n",
        "        minority_up = resample(minority,\n",
        "                               replace=True,\n",
        "                               n_samples=len(majority),\n",
        "                               random_state=random_state)\n",
        "        df_resampled = pd.concat([majority, minority_up], axis=0)\n",
        "    else:  # undersample\n",
        "        majority_down = resample(majority,\n",
        "                                 replace=False,\n",
        "                                 n_samples=len(minority),\n",
        "                                 random_state=random_state)\n",
        "        df_resampled = pd.concat([majority_down, minority], axis=0)\n",
        "\n",
        "    df_resampled = df_resampled.sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n",
        "    if verbose:\n",
        "        counts = df_resampled[target_col].value_counts().to_dict()\n",
        "        print(f\"Resampled class counts ({method}):\", counts)\n",
        "    return df_resampled\n",
        "\n",
        "def preprocess_dataset(df,\n",
        "                       target_col='target',\n",
        "                       remove_constants=True,\n",
        "                       remove_correlated=True,\n",
        "                       corr_threshold=0.95,\n",
        "                       cap_outliers=True,\n",
        "                       outlier_multiplier=1.5,\n",
        "                       resample_method='none',\n",
        "                       random_state=42,\n",
        "                       verbose=True):\n",
        "    \"\"\"\n",
        "    Wrapper that runs a sequence of preprocessing steps. Each step can be toggled.\n",
        "    Returns the processed DataFrame.\n",
        "    \"\"\"\n",
        "    # 1) Basic checks\n",
        "    if verbose:\n",
        "        print(\"Starting preprocessing...\")\n",
        "    df = check_missing_and_duplicates(df, verbose=verbose)\n",
        "\n",
        "    # 2) Drop constant columns\n",
        "    if remove_constants:\n",
        "        df = drop_constant_columns(df, verbose=verbose)\n",
        "\n",
        "    # 3) Remove highly correlated features\n",
        "    if remove_correlated:\n",
        "        df = remove_highly_correlated(df, target_col=target_col, threshold=corr_threshold, verbose=verbose)\n",
        "\n",
        "    # 4) Cap outliers\n",
        "    if cap_outliers:\n",
        "        df = cap_outliers_iqr(df, multiplier=outlier_multiplier, exclude=[target_col], verbose=verbose)\n",
        "\n",
        "    # 5) Resample for class imbalance (if requested)\n",
        "    if resample_method != 'none':\n",
        "        df = resample_classes(df, target_col=target_col, method=resample_method, random_state=random_state, verbose=verbose)\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(\"Skipping resampling step.\")\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Preprocessing complete. Resulting shape:\", df.shape)\n",
        "    return df\n",
        "\n",
        "df_clean = preprocess_dataset(df,\n",
        "                              target_col='target',\n",
        "                              remove_constants=False,\n",
        "                              remove_correlated=False,\n",
        "                              corr_threshold=0.95,\n",
        "                              cap_outliers=False,\n",
        "                              outlier_multiplier=1.5,\n",
        "                              resample_method='none',\n",
        "                              random_state=42,\n",
        "                              verbose=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69f9c2e2",
      "metadata": {
        "id": "69f9c2e2"
      },
      "source": [
        "---\n",
        "##Chapter 3) Set target column and TTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbb76f7",
      "metadata": {
        "id": "1fbb76f7"
      },
      "outputs": [],
      "source": [
        "TARGET_COL = \"target\"  # change for your dataset\n",
        "df[TARGET_COL].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd6fe528",
      "metadata": {
        "id": "bd6fe528"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=[TARGET_COL])\n",
        "y = df[TARGET_COL]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a6a488b",
      "metadata": {
        "id": "3a6a488b"
      },
      "source": [
        "---\n",
        "##Chapter 4) Preprocessing pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a61a13",
      "metadata": {
        "id": "50a61a13"
      },
      "outputs": [],
      "source": [
        "num_cols = X_train.select_dtypes(exclude=[\"object\",\"category\"]).columns\n",
        "cat_cols = X_train.select_dtypes(include=[\"object\",\"category\"]).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", \"passthrough\", num_cols),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "num_cols[:10], cat_cols[:10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f4fb64",
      "metadata": {
        "id": "58f4fb64"
      },
      "source": [
        "---\n",
        "##Chapter 5) Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hwPGZbvWaahg",
      "metadata": {
        "id": "hwPGZbvWaahg"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------\n",
        "# Decision Tree (Classification) + GridSearch (MCC) + Print Tree\n",
        "# ------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.metrics import matthews_corrcoef, make_scorer\n",
        "\n",
        "# Stratified CV keeps class balance similar across splits\n",
        "cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
        "\n",
        "# MCC scorer (higher is better)\n",
        "mcc_scorer = make_scorer(matthews_corrcoef)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 1) Pipeline\n",
        "# -------------------------------------------\n",
        "pipe_dt = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# -------------------------------------------\n",
        "# 2) Small param grid (fast)\n",
        "# -------------------------------------------\n",
        "param_grid_dt = {\n",
        "    \"classifier__max_depth\": [8, 15], #lets use a deeper tree to illustrate the need for Feature Importance\n",
        "    \"classifier__criterion\": [\"gini\", \"entropy\", \"log_loss\"]\n",
        "}\n",
        "\n",
        "# -------------------------------------------\n",
        "# 3) GridSearchCV\n",
        "# -------------------------------------------\n",
        "gs_dt = GridSearchCV(\n",
        "    estimator=pipe_dt,\n",
        "    param_grid=param_grid_dt,\n",
        "    cv=cv,\n",
        "    scoring=mcc_scorer,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -------------------------------------------\n",
        "# 4) Fit\n",
        "# -------------------------------------------\n",
        "gs_dt.fit(X_train, y_train)\n",
        "print(\"Decision Tree grid search complete.\")\n",
        "print(\"Best DT Params:\", gs_dt.best_params_)\n",
        "print(\"Best CV MCC:\", gs_dt.best_score_)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 5) Test MCC\n",
        "# -------------------------------------------\n",
        "dt_best = gs_dt.best_estimator_\n",
        "dt_pred = dt_best.predict(X_test)\n",
        "print(\"\\nTest MCC (Decision Tree):\", matthews_corrcoef(y_test, dt_pred))\n",
        "\n",
        "# -------------------------------------------\n",
        "# 6) Print the tree rules (text)\n",
        "# -------------------------------------------\n",
        "# Get feature names after preprocessing (works for ColumnTransformer in recent sklearn)\n",
        "pre = dt_best.named_steps[\"preprocessor\"]\n",
        "clf = dt_best.named_steps[\"classifier\"]\n",
        "\n",
        "try:\n",
        "    feature_names = pre.get_feature_names_out()\n",
        "except Exception:\n",
        "    feature_names = [f\"f{i}\" for i in range(clf.n_features_in_)]\n",
        "\n",
        "tree_text = export_text(clf, feature_names=list(feature_names), max_depth=6)\n",
        "print(\"\\nDecision Tree (first 6 levels):\\n\")\n",
        "print(tree_text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xk53Mai1bCg0",
      "metadata": {
        "id": "Xk53Mai1bCg0"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------\n",
        "# Optional: plot tree (needs matplotlib)\n",
        "# -------------------------------------------\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(18, 10))\n",
        "plot_tree(clf, feature_names=feature_names, class_names=True, filled=True, max_depth=4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-2s0E1Gdbr25",
      "metadata": {
        "id": "-2s0E1Gdbr25"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------\n",
        "# 7) Feature Importance (FI) from the best Decision Tree\n",
        "# (use clear names so it won't clash with XGB variables)\n",
        "# -------------------------------------------\n",
        "import pandas as pd\n",
        "\n",
        "dt_preproc = dt_best.named_steps[\"preprocessor\"]\n",
        "dt_clf = dt_best.named_steps[\"classifier\"]\n",
        "\n",
        "# Get feature names after preprocessing\n",
        "try:\n",
        "    dt_feature_names = dt_preproc.get_feature_names_out()\n",
        "except Exception:\n",
        "    dt_feature_names = [f\"f{i}\" for i in range(dt_clf.n_features_in_)]\n",
        "\n",
        "# DecisionTreeClassifier has feature_importances_\n",
        "dt_fi = pd.Series(dt_clf.feature_importances_, index=dt_feature_names)\n",
        "\n",
        "# Sort high to low, and drop zeros for readability (optional)\n",
        "dt_fi_sorted = dt_fi.sort_values(ascending=False)\n",
        "dt_fi_nonzero = dt_fi_sorted[dt_fi_sorted > 0]\n",
        "\n",
        "print(\"\\nTop Feature Importances (Decision Tree):\")\n",
        "print(dt_fi_nonzero.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ign4xrDzjY5N",
      "metadata": {
        "id": "Ign4xrDzjY5N"
      },
      "source": [
        "Notes:\n",
        "- Decision Tree FI values sum to 1.0 within THIS trained tree.\n",
        "- FI is based on total impurity (or loss) reduction contributed by each feature.\n",
        "- Adding FI from different runs (or different CV folds) does not mean anything by itself.\n",
        "- Comparing FI across different models is not directly meaningful because each model computes FI differently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e64311e4",
      "metadata": {
        "id": "e64311e4"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------\n",
        "# 0. NOTE: This block takes quite a while to run, do it before moving onto explanation of code\n",
        "# ------------------------------------\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# For classification, prefer stratified splits so class balance is similar in each split\n",
        "cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 1. Create pipelines for both models\n",
        "# -------------------------------------------\n",
        "\n",
        "pipe_rf = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "pipe_xgb = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", XGBClassifier(\n",
        "        random_state=42,\n",
        "        eval_metric=\"logloss\"  # sensible default for binary classification\n",
        "    ))\n",
        "])\n",
        "\n",
        "# -------------------------------------------\n",
        "# 2. Define parameter grids\n",
        "# Keep them small for speed and simplicity\n",
        "# -------------------------------------------\n",
        "\n",
        "param_grid_rf = {\n",
        "    \"classifier__n_estimators\": [50, 200],\n",
        "    \"classifier__max_depth\": [5, 10],\n",
        "    \"classifier__criterion\": [\"gini\", \"entropy\", \"log_loss\"]\n",
        "}\n",
        "\n",
        "param_grid_xgb = {\n",
        "    \"classifier__n_estimators\": [50, 200],\n",
        "    \"classifier__max_depth\": [2, 4, 6],\n",
        "    \"classifier__eval_metric\": [\"logloss\", \"auc\"]\n",
        "}\n",
        "\n",
        "# -------------------------------------------\n",
        "# 3. Create GridSearchCV objects\n",
        "# -------------------------------------------\n",
        "#make scorer mcc\n",
        "from sklearn.metrics import make_scorer, matthews_corrcoef\n",
        "\n",
        "\n",
        "gs_rf = GridSearchCV(\n",
        "    estimator=pipe_rf,\n",
        "    param_grid=param_grid_rf,\n",
        "    cv=cv,\n",
        "    scoring=\"matthews_corrcoef\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "gs_xgb = GridSearchCV(\n",
        "    estimator=pipe_xgb,\n",
        "    param_grid=param_grid_xgb,\n",
        "    cv=cv,\n",
        "    scoring=\"matthews_corrcoef\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -------------------------------------------\n",
        "# 4. Fit both models\n",
        "# (Students can run one at a time if needed)\n",
        "# -------------------------------------------\n",
        "\n",
        "gs_rf.fit(X_train, y_train)\n",
        "print(\"Random Forest grid search complete.\")\n",
        "\n",
        "gs_xgb.fit(X_train, y_train)\n",
        "print(\"XGBoost grid search complete.\")\n",
        "\n",
        "# -------------------------------------------\n",
        "# 5. Evaluate on test set\n",
        "# -------------------------------------------\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, matthews_corrcoef\n",
        "\n",
        "rf_pred = gs_rf.best_estimator_.predict(X_test)\n",
        "xgb_pred = gs_xgb.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"\\nRF MCC:\", matthews_corrcoef(y_test, rf_pred))\n",
        "print(\"Best RF Params:\", gs_rf.best_params_)\n",
        "print(\"\\nRF Classification Report:\\n\", classification_report(y_test, rf_pred))\n",
        "print(\"RF Confusion Matrix:\\n\", confusion_matrix(y_test, rf_pred))\n",
        "\n",
        "print(\"\\nXGB MCC:\", matthews_corrcoef(y_test, xgb_pred))\n",
        "print(\"Best XGB Params:\", gs_xgb.best_params_)\n",
        "print(\"\\nXGB Classification Report:\\n\", classification_report(y_test, xgb_pred))\n",
        "print(\"XGB Confusion Matrix:\\n\", confusion_matrix(y_test, xgb_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YFzkcN0haIKT",
      "metadata": {
        "id": "YFzkcN0haIKT"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------\n",
        "# 6) Feature Importance (FI) for the best XGBoost model (from GridSearch)\n",
        "# (use clear names so it won't clash with DT variables)\n",
        "# -------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "xgb_best_pipe = gs_xgb.best_estimator_\n",
        "xgb_preproc = xgb_best_pipe.named_steps[\"preprocessor\"]\n",
        "xgb_clf = xgb_best_pipe.named_steps[\"classifier\"]\n",
        "\n",
        "# Feature names after preprocessing\n",
        "try:\n",
        "    xgb_feature_names = xgb_preproc.get_feature_names_out()\n",
        "except Exception:\n",
        "    xgb_feature_names = np.array([f\"f{i}\" for i in range(xgb_clf.n_features_in_)], dtype=object)\n",
        "\n",
        "# XGBoost importance types:\n",
        "# - \"gain\": average gain of splits using the feature (closest to tree FI conceptually)\n",
        "# - \"weight\": number of times feature is used in splits\n",
        "# - \"cover\": average coverage (samples affected) of splits using the feature\n",
        "xgb_importance_type = \"gain\"\n",
        "\n",
        "# Get raw importances keyed by \"f0\", \"f1\", ...\n",
        "xgb_raw = xgb_clf.get_booster().get_score(importance_type=xgb_importance_type)\n",
        "\n",
        "# Convert to aligned Series for all features (missing -> 0)\n",
        "xgb_fi = pd.Series(0.0, index=xgb_feature_names)\n",
        "for k, v in xgb_raw.items():\n",
        "    # k looks like \"f12\" -> index 12\n",
        "    idx = int(k[1:])\n",
        "    if idx < len(xgb_feature_names):\n",
        "        xgb_fi.iloc[idx] = float(v)\n",
        "\n",
        "xgb_fi_sorted = xgb_fi.sort_values(ascending=False)\n",
        "\n",
        "print(f\"\\nTop XGBoost Feature Importances (importance_type='{xgb_importance_type}'):\")\n",
        "print(xgb_fi_sorted.head(20))\n",
        "\n",
        "print(\"\\nNotes:\")\n",
        "print(\"- XGBoost FI depends on importance_type (gain/weight/cover).\")\n",
        "print(\"- These values are not on the same scale as Decision Tree FI (Decision Tree sums to 1).\")\n",
        "print(\"- Adding FI numbers across different runs/folds does not make sense.\")\n",
        "print(\"- Comparing FI across different model families is not directly meaningful.\")\n",
        "\n",
        "# Optional: show other importance types for comparison (top 10 each)\n",
        "for t in [\"weight\", \"cover\"]:\n",
        "    xgb_raw_t = xgb_clf.get_booster().get_score(importance_type=t)\n",
        "\n",
        "    xgb_fi_t = pd.Series(0.0, index=xgb_feature_names)\n",
        "    for k, v in xgb_raw_t.items():\n",
        "        idx = int(k[1:])\n",
        "        if idx < len(xgb_feature_names):\n",
        "            xgb_fi_t.iloc[idx] = float(v)\n",
        "\n",
        "    print(f\"\\nTop XGBoost FI (importance_type='{t}'):\")\n",
        "    print(xgb_fi_t.sort_values(ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15uR54wvfi15",
      "metadata": {
        "id": "15uR54wvfi15"
      },
      "source": [
        "---\n",
        "\n",
        "##Permutation Feature Importance (pFI): Why you still need it?\n",
        "\n",
        "You have seen that tree-based FI is based on **internal split improvements**.\n",
        "\n",
        "That is useful, but it answers only this question:\n",
        "\n",
        "> “Which features helped the model make good splits *inside the tree*?”\n",
        "\n",
        "This is **not the same** as:\n",
        "\n",
        "> “Which features actually matter for prediction *after the model is trained*?”\n",
        "\n",
        "This gap is exactly why **pFI is needed**.\n",
        "\n",
        "---\n",
        "\n",
        "## The key limitation of tree-based FI (from your context)\n",
        "\n",
        "From your explanation, tree FI has these properties:\n",
        "\n",
        "* It is based on **how the model was built**\n",
        "* It depends on **how often and how strongly features were used in splits**\n",
        "* It reflects **internal optimisation**, not real-world dependence\n",
        "\n",
        "This leads to known issues:\n",
        "\n",
        "### 1. Bias towards certain features\n",
        "\n",
        "Tree FI tends to favour:\n",
        "\n",
        "* continuous features over categorical ones\n",
        "* features with many possible split points\n",
        "\n",
        "So a feature may rank high in FI **because it was convenient to split on**, not because it is truly necessary.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Correlated features confuse FI\n",
        "\n",
        "If two features carry similar information:\n",
        "\n",
        "* the tree may choose one early\n",
        "* the other appears less often\n",
        "* FI makes the second feature look unimportant\n",
        "\n",
        "But in reality:\n",
        "\n",
        "* removing either one would hurt performance\n",
        "\n",
        "Tree FI cannot show this properly because it only sees **chosen splits**, not **what would happen if a feature disappeared**.\n",
        "\n",
        "---\n",
        "\n",
        "## What permutation FI (pFI) measures instead\n",
        "\n",
        "**Standard definition (conceptual):**\n",
        "\n",
        "Permutation FI measures:\n",
        "\n",
        "> “How much worse does the model perform if I break the relationship between this feature and the target?”\n",
        "\n",
        "How it works, at a high level:\n",
        "\n",
        "1. Keep the trained model fixed <br>\n",
        "2. **Randomly shuffle the values from one feature column** (hence the name: permutation) <br>\n",
        "3. Measure how much the model’s performance **drops**\n",
        "4. Repeat for each feature\n",
        "5. Hence, it is independent of type of model used (model-agnostic)\n",
        "\n",
        "So pFI directly measures **dependence of predictions on a feature**, not how the model was constructed.\n",
        "\n",
        "---\n",
        "\n",
        "## Why pFI complements FI (not replaces it)\n",
        "\n",
        "| Tree FI                            | Permutation FI                      |\n",
        "| ---------------------------------- | ----------------------------------- |\n",
        "| Based on split improvements        | Based on performance degradation    |\n",
        "| Model-internal                     | Model-agnostic                      |\n",
        "| Explains *how the model was built* | Explains *what the model relies on* |\n",
        "| Can be biased by split mechanics   | Less sensitive to split bias        |\n",
        "| Ordinal ranking only               | Ordinal ranking only                |\n",
        "\n",
        "From your earlier section, this is important:\n",
        "\n",
        "> FI is a true ordinal ranking of split-improvement contributions\n",
        "\n",
        "pFI gives you **another ordinal ranking**, but from a **different question**:\n",
        "\n",
        "> “If I disturb this feature, how much does the model suffer?”\n",
        "\n",
        "When both rankings agree, confidence is high.\n",
        "When they disagree, that disagreement itself is informative.\n",
        "\n",
        "---\n",
        "\n",
        "## Why pFI fits naturally after your FI explanation\n",
        "\n",
        "Given your points that:\n",
        "\n",
        "* FI numbers are not additive\n",
        "* FI numbers are model-dependent\n",
        "* FI is not a true effect size\n",
        "\n",
        "pFI helps because:\n",
        "\n",
        "* it is tied to **evaluation metrics** (MCC, accuracy, MAE, etc.)\n",
        "* it operates on **held-out data**\n",
        "* it reflects **prediction sensitivity**, not split mechanics\n",
        "\n",
        "But the same warnings still apply:\n",
        "\n",
        "* pFI values are still **relative**\n",
        "* they are **ordinal**, not absolute\n",
        "* they depend on the chosen metric\n",
        "\n",
        "So pFI does **not fix everything**, but it answers a **different and more practical question**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kIhENj1EeX8B",
      "metadata": {
        "id": "kIhENj1EeX8B"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------\n",
        "# 8) Permutation Feature Importance (pFI)\n",
        "# What mattered most to prediction accuracy\n",
        "# -------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import matthews_corrcoef, make_scorer\n",
        "\n",
        "# Use MCC to stay consistent with model selection\n",
        "mcc_scorer = make_scorer(matthews_corrcoef)\n",
        "\n",
        "# IMPORTANT:\n",
        "# - pFI must be computed on held-out data\n",
        "# - the model must already be trained\n",
        "# - we do NOT refit the model here\n",
        "\n",
        "# Compute permutation importance on the TEST set\n",
        "pfi = permutation_importance(\n",
        "    estimator=dt_best,\n",
        "    X=X_test,\n",
        "    y=y_test,\n",
        "    scoring=mcc_scorer,\n",
        "    n_repeats=10,          # repeat permutations for stability\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Mean drop in score across repeats\n",
        "pfi_mean = pd.Series(\n",
        "    pfi.importances_mean,\n",
        "    index=feature_names\n",
        ")\n",
        "\n",
        "# Optional: variability across repeats (for teaching discussion)\n",
        "pfi_std = pd.Series(\n",
        "    pfi.importances_std,\n",
        "    index=feature_names\n",
        ")\n",
        "\n",
        "# Sort by importance (largest performance drop first)\n",
        "pfi_sorted = pfi_mean.sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nTop Permutation Feature Importances (Decision Tree, MCC drop):\")\n",
        "print(pfi_sorted.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ntAglJDjkKK",
      "metadata": {
        "id": "9ntAglJDjkKK"
      },
      "source": [
        "Notes:\n",
        "- pFI measures how much MCC drops when a feature is permuted.\n",
        "- Larger values mean prediction accuracy depends more on that feature.\n",
        "- pFI values are relative and ordinal, not additive or absolute.\n",
        "- A low pFI does not mean a feature is useless; it may be redundant with others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6NRPbB7BhVDA",
      "metadata": {
        "id": "6NRPbB7BhVDA"
      },
      "outputs": [],
      "source": [
        "# Build comparison table\n",
        "dt_fi_vs_pfi = pd.DataFrame({\n",
        "    \"DT_FI (split-based)\": dt_fi,\n",
        "    \"DT_pFI (MCC_drop)\": pfi_mean\n",
        "})\n",
        "\n",
        "# Sort by pFI (what matters most to accuracy)\n",
        "dt_fi_vs_pfi_sorted = dt_fi_vs_pfi.sort_values(\n",
        "    by=\"DT_pFI (MCC_drop)\",\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "print(\"\\nDecision Tree: FI vs Permutation FI (sorted by pFI):\")\n",
        "print(dt_fi_vs_pfi_sorted.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TaTHqo8fjy1N",
      "metadata": {
        "id": "TaTHqo8fjy1N"
      },
      "source": [
        "How to read this table:\n",
        "- DT_FI shows which features the tree used most during training.\n",
        "- DT_pFI shows which features the trained model depends on for accuracy.\n",
        "- High FI + high pFI: core, non-redundant signal.\n",
        "- High FI + low pFI : used by the tree, but largely replaceable.\n",
        "- Low FI + high pFI : rarely used in splits, but crucial for predictions.\n",
        "- Low FI + low pFI  : weak or redundant features.\n",
        "\n",
        "Important reminders:\n",
        "- Both FI and pFI are ordinal rankings, not precise measurements.\n",
        "- Do not add or average FI or pFI values.\n",
        "- Disagreement between FI and pFI is informative, not an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J5NqS1h4j4xY",
      "metadata": {
        "id": "J5NqS1h4j4xY"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Lab 6 Export Pack (for SHAP next session)\n",
        "# sklearn 1.8 SAFE VERSION\n",
        "# Saves artefacts to ./Lab6 so you can push to GitHub\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "LAB_DIR = \"Lab6\"\n",
        "os.makedirs(LAB_DIR, exist_ok=True)\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Pick the best models\n",
        "# ----------------------------\n",
        "# Pipelines: preprocessor + classifier\n",
        "dt_best_pipe  = dt_best\n",
        "xgb_best_pipe = gs_xgb.best_estimator_\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Save fitted model pipelines (sklearn 1.8 only)\n",
        "# ----------------------------\n",
        "joblib.dump(\n",
        "    dt_best_pipe,\n",
        "    os.path.join(LAB_DIR, \"dt_best_pipeline_sklearn180.joblib\")\n",
        ")\n",
        "joblib.dump(\n",
        "    xgb_best_pipe,\n",
        "    os.path.join(LAB_DIR, \"xgb_best_pipeline_sklearn180.joblib\")\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Save background sample for SHAP (fixed, small)\n",
        "# ----------------------------\n",
        "RANDOM_STATE = 42\n",
        "BG_N = 300   # keep small for speed + stability\n",
        "\n",
        "X_train_df = (\n",
        "    X_train.copy()\n",
        "    if isinstance(X_train, pd.DataFrame)\n",
        "    else pd.DataFrame(X_train)\n",
        ")\n",
        "y_train_ser = (\n",
        "    y_train.copy()\n",
        "    if isinstance(y_train, (pd.Series, pd.DataFrame))\n",
        "    else pd.Series(y_train, name=\"y\")\n",
        ")\n",
        "\n",
        "X_bg = X_train_df.sample(\n",
        "    n=min(BG_N, len(X_train_df)),\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "X_bg.to_csv(os.path.join(LAB_DIR, \"X_background.csv\"), index=False)\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Save feature names after preprocessing (for SHAP plots)\n",
        "# ----------------------------\n",
        "def safe_get_feature_names(pipe):\n",
        "    pre = pipe.named_steps.get(\"preprocessor\", None)\n",
        "    if pre is None:\n",
        "        return None\n",
        "    try:\n",
        "        names = pre.get_feature_names_out()\n",
        "        return [str(x) for x in names]\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "dt_feature_names  = safe_get_feature_names(dt_best_pipe)\n",
        "xgb_feature_names = safe_get_feature_names(xgb_best_pipe)\n",
        "\n",
        "if dt_feature_names is not None:\n",
        "    pd.Series(dt_feature_names, name=\"feature\").to_csv(\n",
        "        os.path.join(LAB_DIR, \"dt_feature_names.csv\"),\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "if xgb_feature_names is not None:\n",
        "    pd.Series(xgb_feature_names, name=\"feature\").to_csv(\n",
        "        os.path.join(LAB_DIR, \"xgb_feature_names.csv\"),\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Save sanity-check predictions + MCC\n",
        "# ----------------------------\n",
        "X_test_df = (\n",
        "    X_test.copy()\n",
        "    if isinstance(X_test, pd.DataFrame)\n",
        "    else pd.DataFrame(X_test)\n",
        ")\n",
        "y_test_ser = (\n",
        "    y_test.copy()\n",
        "    if isinstance(y_test, (pd.Series, pd.DataFrame))\n",
        "    else pd.Series(y_test, name=\"y\")\n",
        ")\n",
        "\n",
        "dt_pred  = dt_best_pipe.predict(X_test)\n",
        "xgb_pred = xgb_best_pipe.predict(X_test)\n",
        "\n",
        "dt_mcc  = matthews_corrcoef(y_test_ser, dt_pred)\n",
        "xgb_mcc = matthews_corrcoef(y_test_ser, xgb_pred)\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"dt_pred\": dt_pred,\n",
        "    \"xgb_pred\": xgb_pred\n",
        "}).to_csv(os.path.join(LAB_DIR, \"test_predictions.csv\"), index=False)\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"model\": [\"DecisionTree\", \"XGBoost\"],\n",
        "    \"test_mcc\": [dt_mcc, xgb_mcc]\n",
        "}).to_csv(os.path.join(LAB_DIR, \"test_mcc.csv\"), index=False)\n",
        "\n",
        "with open(os.path.join(LAB_DIR, \"README_sanity_check.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\n",
        "        \"Sanity check (sklearn 1.8):\\n\"\n",
        "        \"- Load pipeline(s) saved with sklearn 1.8\\n\"\n",
        "        \"- Recompute predictions on the SAME X_test\\n\"\n",
        "        \"- Confirm MCC matches test_mcc.csv before SHAP\\n\"\n",
        "        \"- Do NOT load these models under older sklearn versions\\n\"\n",
        "    )\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Save split indices (recommended for reproducibility)\n",
        "# ----------------------------\n",
        "pd.Series(\n",
        "    X_train_df.index,\n",
        "    name=\"train_idx\"\n",
        ").to_csv(os.path.join(LAB_DIR, \"train_idx.csv\"), index=False)\n",
        "\n",
        "pd.Series(\n",
        "    X_test_df.index,\n",
        "    name=\"test_idx\"\n",
        ").to_csv(os.path.join(LAB_DIR, \"test_idx.csv\"), index=False)\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Save library versions (critical for model loading)\n",
        "# ----------------------------\n",
        "import sklearn\n",
        "import shap\n",
        "import xgboost\n",
        "\n",
        "versions = {\n",
        "    \"python\": f\"{os.sys.version_info.major}.{os.sys.version_info.minor}.{os.sys.version_info.micro}\",\n",
        "    \"sklearn\": sklearn.__version__,\n",
        "    \"xgboost\": xgboost.__version__,\n",
        "    \"shap\": shap.__version__,\n",
        "    \"pandas\": pd.__version__,\n",
        "    \"numpy\": np.__version__,\n",
        "}\n",
        "\n",
        "with open(os.path.join(LAB_DIR, \"VERSIONS.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(versions, f, indent=2)\n",
        "\n",
        "print(f\"Export complete (sklearn 1.8). Files written to: ./{LAB_DIR}\")\n",
        "print(\"Key files:\")\n",
        "print(\"- dt_best_pipeline_sklearn180.joblib\")\n",
        "print(\"- xgb_best_pipeline_sklearn180.joblib\")\n",
        "print(\"- X_background.csv\")\n",
        "print(\"- *feature_names.csv (if available)\")\n",
        "print(\"- test_predictions.csv, test_mcc.csv\")\n",
        "print(\"- train_idx.csv, test_idx.csv\")\n",
        "print(\"- VERSIONS.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wbkRPLm1ncgX",
      "metadata": {
        "id": "wbkRPLm1ncgX"
      },
      "source": [
        "##Create your Github's Fine-grained personal access token\n",
        "\n",
        "1. Go to your GitHub settings\n",
        "\n",
        "2. Top-right profile icon → Settings\n",
        "\n",
        "3. Open Developer settings (at bottom of left side bar)\n",
        "\n",
        "4. Personal access tokens → Fine-grained tokens\n",
        "\n",
        "5. Click Generate new token\n",
        "\n",
        "6. Fill in token basics\n",
        "\n",
        ">Token name (e.g., colab-push-lab6)\n",
        ">\n",
        "> Expiration: pick a duration, for example 90 days (fine-grained tokens are designed to expire)\n",
        ">\n",
        ">Resource owner: usually your own account (or choose an org if needed)\n",
        ">\n",
        ">Set Repository access\n",
        ">\n",
        ">Choose Only select repositories\n",
        ">Select: `your account`/ADALL_github\n",
        ">\n",
        ">Set Permissions\n",
        ">\n",
        ">Under Repository permissions:\n",
        ">\n",
        ">Check Contents → Select Read and write (this is the key permission to push commits)\n",
        "\n",
        "7. Click Generate token\n",
        "\n",
        "Copy the token immediately. You will not be able to view it again later (you can only regenerate a new one)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f67a79c",
      "metadata": {
        "id": "0f67a79c"
      },
      "source": [
        "## GitHub: Save `Lab6/` artefacts into repo and push (Colab)\n",
        "\n",
        "This section does only GitHub steps:\n",
        "1) Load your GitHub PAT from **Colab Secrets** into an environment variable  \n",
        "2) Clone the repo if needed  \n",
        "3) Copy `/content/Lab6` into the repo as `Lab6/`  \n",
        "4) Commit and push, then reset the remote URL (so token is not kept in the repo config)\n",
        "\n",
        "> You must create a Colab Secret named `GITHUB_TOKEN` first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81ce2547",
      "metadata": {
        "id": "81ce2547"
      },
      "outputs": [],
      "source": [
        "# Load PAT from Colab Secrets and expose it to bash cells (do NOT print it)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "token = userdata.get(\"GITHUB_TOKEN\")\n",
        "assert token, \"Missing Colab Secret: GITHUB_TOKEN\"\n",
        "\n",
        "os.environ[\"GITHUB_TOKEN\"] = token\n",
        "os.environ[\"GITHUB_USER\"] = \"rq-goh\" #replace with your GitHub username\n",
        "os.environ[\"GITHUB_REPO\"] = \"ADALL_github\" #replace with your GitHub repo\n",
        "\n",
        "print(\"GitHub token loaded into environment (not printed).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1232fed4",
      "metadata": {
        "id": "1232fed4"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -e\n",
        "\n",
        "REPO_DIR=\"/content/${GITHUB_REPO}\"\n",
        "\n",
        "echo \"Repo dir: $REPO_DIR\"\n",
        "\n",
        "# 1) Ensure repo is cloned\n",
        "if [ ! -d \"$REPO_DIR/.git\" ]; then\n",
        "  echo \"Cloning repo into $REPO_DIR\"\n",
        "  cd /content\n",
        "  git clone \"https://${GITHUB_USER}:${GITHUB_TOKEN}@github.com/${GITHUB_USER}/${GITHUB_REPO}.git\"\n",
        "else\n",
        "  echo \"Repo already exists at $REPO_DIR\"\n",
        "fi\n",
        "\n",
        "# 2) Ensure Lab6 exists\n",
        "if [ ! -d \"/content/Lab6\" ]; then\n",
        "  echo \"ERROR: /content/Lab6 not found. Run your export code first.\"\n",
        "  exit 1\n",
        "fi\n",
        "\n",
        "# 3) Copy artefacts into repo/Lab6\n",
        "mkdir -p \"$REPO_DIR/Lab6\"\n",
        "cp -rf /content/Lab6/* \"$REPO_DIR/Lab6/\"\n",
        "\n",
        "echo \"---- repo/Lab6 (first 80 lines) ----\"\n",
        "ls -lah \"$REPO_DIR/Lab6\" | head -n 80\n",
        "\n",
        "# 4) Commit & push (then reset remote URL to avoid token leakage)\n",
        "git config --global user.name \"${GITHUB_USER}\"\n",
        "git config --global user.email \"your_email@example.com\"\n",
        "\n",
        "# Set token remote just for pushing\n",
        "git -C \"$REPO_DIR\" remote set-url origin \"https://${GITHUB_USER}:${GITHUB_TOKEN}@github.com/${GITHUB_USER}/${GITHUB_REPO}.git\"\n",
        "\n",
        "git -C \"$REPO_DIR\" add Lab6\n",
        "git -C \"$REPO_DIR\" commit -m \"Lab6: add artefacts for SHAP session\" || echo \"No changes to commit\"\n",
        "git -C \"$REPO_DIR\" push\n",
        "\n",
        "# Reset remote back to clean URL\n",
        "git -C \"$REPO_DIR\" remote set-url origin \"https://github.com/${GITHUB_USER}/${GITHUB_REPO}.git\"\n",
        "\n",
        "echo \"Done: pushed to GitHub and remote reset.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8120ff",
      "metadata": {
        "id": "7f8120ff"
      },
      "source": [
        "### Raw file URL format\n",
        "\n",
        "`https://raw.githubusercontent.com/rq-goh/ADALL_github/refs/heads/main/Lab6/<filename>`\n",
        "\n",
        "If GitHub rejects the push due to file size, check file sizes:\n",
        "\n",
        "```bash\n",
        "!du -h /content/ADALL_github/Lab6/*\n",
        "```\n",
        "\n",
        "Then switch to Git LFS.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print versions of library used\n",
        "print(sklearn.__version__)\n",
        "print(xgboost.__version__)\n",
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "#"
      ],
      "metadata": {
        "id": "5rVpCYJgV-6B"
      },
      "id": "5rVpCYJgV-6B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwuae3DaWZpI"
      },
      "id": "kwuae3DaWZpI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}